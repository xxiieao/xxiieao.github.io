---
layout: post
title:  "Shannon Entropy"
date: 2016-04-09 14:04:13 +0800
categories: "math"
author: xxiieao
---

上个星期翼飞分享了霍夫曼编码(Huffman Coding)，这是一种非常有用编码技术，能非常大程度对数据进行压缩。数据压缩是否有极限，这个极限是多少？霍夫曼编码离这个极限有多远？

解答这个问题之前，我们需要先了解一个重要的概念——信息熵（Entropy）。学过物理的人会注意到信息熵的英文术语Entorpy和熵是一样，而实际上二者在概念上也有相似之处。物理上的熵用来衡量一个系统的混乱程度，系统的混乱程度越大（粒子分布越均匀），熵值越大。信息学上的熵则用来衡量信息中包含的信息量，同时感谢信息论的师祖——香浓给出了定量公式

$$ H(X) = - \sum_{i=1}^n P(x_i) log_bP(x_i) $$

其中，H(X)就是信息熵，E表示取平均，P(x)是事件x发生的概率。当对数的底取2，H的单位被称为bit。

那么这个熵代表的信息量到底是什么意思，和我们平时说的信息量有什么不同？我们举个例子来解释。比如今天晚上有一场足球赛——中国队对巴西队，由于某些原因你无法观看比赛，于是你和朋友约定比赛结束的时候让他给你发一条信息告诉你结果。那么这条信息的信息量是多少？本着不黑不捧的原则，我们估计中国胜利和打平的概率是0%，计算该信息的熵值的具体算式如下

$$ H = -1 * ( P_{中国胜} * log_2 P_{中国胜} + P_{打平} * log_2 P_{打平} + P_{巴西胜} * log_2 P_{巴西胜} )  = 0 $$

嗯，信息量为0，因为地球人都知道中国必输，比赛结果根本没有必要发送。我们把比赛场次换换，例如德国对西班牙，假设德国胜，打平，西班牙胜的三种结果的概率分别是0.5，0.3，0.2。那么朋友发给我的信息的信息量如下

$$ H = -1 * (0.5 * log_2 0.5 + 0.3 * log_2 0.3 + 0.2 * log_2 0.2 ) = 1.49 $$

怎么理解这个1.49呢，朋友要把结果发给我，我们俩可以约定好一个简单的编码例如00表示德国胜，01表示打平，10表示西班牙胜。这样朋友只要发两个bit的数据就能搞定了，可喜可贺。这种无论结果怎样都发同样长度数据的做法其实就是定长编码啦。那能不能再省点流量费呢？当然可以，显然现在的比赛德国赢面大啊，于是我们可以重新商定一个编码规则，0表示德国赢，10表示打平，11表示西班牙赢。那么这样如果德国胜利我只要发一个bit，其它情况发两个bit，由于德国胜率是0.5，那么我发送数据平均长度是1.5个bit，这种就是我们常用的变长编码。看到这里，你不觉得1.5和1.49好接近吗？没错，信息熵实际上就是信息编码的最小长度。有人可能要问，为啥不用0表示德国胜利，1表示打平，01表示西班牙胜利，这样只要1.2个bit。这是因为使用了变长编码，从信息传输的角度来说，你必须有方法确定你信息是完整的。在上面的例子中当你收到一个0的时候，你无法确定你是否收到了信息全文（有可能代表德国胜，也有可能你还没收到后面的1，即西班牙胜）。

回到霍夫曼编码上来，假设一个文本有ABCD四个字符，各自有出现了50次，25次，15次和10次。可以得到该文本信息量为

$$ H = -1 * 100 * (0.5 * log_2 0.5 + 0.25 * log_2 0.25 + 0.15 * log_2 0.15 + 0.1 * log_2 0.1) = 174.3 $$

也就是说，最优状况下我们只要175个bit可以储存这段文本。根据霍夫曼编码，A的编码为0，B为10，C为110，D为111。实际上的编码长度为1\*50+2\*25+3\*25=175。嗯？！看来，霍夫曼编码的确不愧是最接近压缩极限的编码方式。
