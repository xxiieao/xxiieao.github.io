---
layout: post
title:  "ANN and Deep Learning Part 3"
date: 2016-03-04 22:03:39 +0800
categories: ""
author: xxiieao
---

深度学习是一个非常热的词，从本质上来说依然是机器学习的分支，一定程度上也可以认为是神经网络的一种类型。不过在一般的语境下神经网络通常是指上一篇文章中提到的多层感知器，深度学习则是指的完全不同的一种类型。由于我自己学习深度学习的时间也不长，认识的深度也不够，因此也是通过几个关键词来描述深度学习。

### 深度学习的基本结构

深度学习实际上是对神经网络的再度开发，因此其基本结构也继承于神经网络。由节点以及连接节点的连接构成运算网络。传统神经网络的隐含层数大多为1～3层，超过这个层数的话，模型的精度并不会得到显著的提高，而训练速度却会显著下降。与之相比，深度学习的隐含层往往在5层以上。这正是深度学习名称中$$深度$$的一点。

如果传统神经网络在层数较多的时候有着性能降低这个问题，为何拥有同样结构的深度学习却没有？回答这个问题之前，先来回答另一个问题——为何深度学习需要如此多的隐含层？

### 新认知机与生物视觉理论

Hubel和Wiesel是1981年的诺贝尔生理或医学奖得主，视皮层(Visual Cortex)正是他们的研究对象。视皮层对视觉信号的加工是分层进行的。其中初级视皮层（primary visual cortex），只负责感应光的强弱，色彩等信号，有点像相机的感光元件。而到了次级视觉层（secondary visual cortex）则会将初级视皮层采集到信号进行加工，获得一些更高阶的信息——例如图形的边缘、双眼图像视差等等。而这些加工过的信息还会继续传递给更高的视皮层继续加工，直到将信息转换成一些更抽象的概念。

正是这个视皮层的信号处理模型，启发了福岛邦彦从而提出了新认知机（Neocognitron，深度学习的模型的一种）模型。该模型的目的就是模仿视觉信号的处理过程，先从数据中提取初级特征，再从初级特征中提取更高级的特征从而达到模式识别的目的。可以发现，如果需要获得某个层次的特征，就必须依赖这样一种层层总结的过程。因此为了获得一些高级特征，深度学习必须具有的多层网络结构。

### wake-sleep algorithm

了解了深度学习的基本框架之后。下面要解决的问题，如何对深度学习模型进行训练，或者说初始化。在神经网络中，首先是随机生成连接权重值，然后输入训练样本进行计算，获得预期结果后计算与真实结果之间的误差，再使用反向传播算法（BP， backpropagation）将这个误差沿着网络反向传播回去，使用梯度下降调整各连接权重。这套系统的最大瓶颈正是BP算法，如果层数较多，在反向传播的过程当中，每经过一层误差会逐渐缩小，等到传播到输入层附近时，误差可能已经小到无法驱动权重进行改变了。这种现象被称之为梯度扩散。

梯度扩散还只是神经网络的问题之一，另外一个比较大的问题是——训练神经网络必须使用标记数据，也就是所谓“有监督学习”。如果一个算法无法突破“监督学习”这个门槛，在人工智能方面是很难有所发展的。

Wake-Sleep Algorithm是深度学习解决以上问题的一个方案（话说这个算法的名字和深度算法一样，光看名字简直高端大气上档次）。这个算法需要的网络结构大致如下（图片来自wiki）

![picture](http://ww4.sinaimg.cn/mw690/6daafd01gw1f1m2pzl79xj2064076748.jpg)

data指输入的数据，R连接指认知连接（recognition connection），G连接指创造连接（Generative connections）。Wake-Sleep Algorithm由Wake阶段和Sleep阶段构成。以data层与layer1层为例子，其中data层可以看作输入，而layer1层可以看作输出。

在Wake阶段，认知连接是固定的，layer1的数据通过创造连接恢复成data层里储存的数据，并且根据误差调整创造连接，使得输出数据能够被正确恢复成输入。

在Sleep阶段，创造连接是固定的，data的层的数据通过认知连接传入layer1中，layer1的数据再通过创造连接重新传回到data层。在这里，如果重新传回的数据和data的数据不一致的话，则通过误差调整认知连接使得二者一致。

在训练好data层与layer1层之间的连接后，就可以继续训练layer1与layer2之间的连接，这个时候layer1充当输入层，layer2充当输出层。

从上面，不难看出这个算法有两个特征，一个每一层的训练都是独立进行的，不存在误差梯度扩散问题。其次训练的标记就是输入本身，也就是说这是个无监督算法。你不需要额外对数据进行标记。保证输入能被正确还原为输出层的设计，实际上是基于这样一种哲学，如果对数据的特征提取正确，那么这些特征也应当能够最大限度地还原出原数据。

打个比方，如果有一组坐标数据，代表的是平面坐标系上的一个圆。我们可以假设正确地（通过认知连接）提取了这些数据的特征，即圆心坐标以及半径长度，那么我们也可以通过特征（通过创造连接）还原出这些坐标数据。

### 关于深度学习

深度学习之深，一是在其网络结构之深，二在于其特征提取之深。深度学习通过特征的提取这一过程去掉原数据集中多余与不重要的数据，因此与神经网络进行多维转换的思维有着本质上的不一样。不过和神经网络一样，深度学习的问题在于其有效性还没能被证明。神经网络和深度学习到底是如何工作的仍然是一片不明朗的领域。
