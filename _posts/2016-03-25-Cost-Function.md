---
layout: post
title:  "Cost Function"
date: 2016-03-25 23:03:23 +0800
categories: "algorithm"
author: xxiieao
---

上一篇文章中谈到了机器学习有两个非常重要的概念，一个是梯度下降（Gradient Descent），另一个是代价函数（cost function）。实际上代价函数的设计是非常重要的：第一点就是代价函数是否准确地体现了机器学习的目标，第二点则是代价函数是否可以方便地进行梯度下降计算。文中以最常用的两种代价函数——线性函数和逻辑斯蒂函数来说明这个问题。

### 线性函数

线性函数主要是用来处理连续变量的。继续以上一篇文章中的线性回归作为例子来说明——假设有一组自变量x和一组因变量y，我们试图构造如下一个模型来将x和y匹配起来

$$ f(a, b) = a * x + b $$

> 不要奇怪为何函数里面的变量是a和b，因为x和y的数值是固定，我们的目的是调整a和b大小使得函数符合要求，因此a和b才是真正的变量

当然我们知道，并不可能让模型f(a, b)的预测值与y刚好相等，这种情况下只好退而求其次让预测值与y的欧式距离尽量得小

$$ d = \dfrac{1}{n} * \sum{ (f(a, b) - y)^2 } $$

> 即让d的值最小（其中n是x和y的个数）。

前面我们知道梯度下降就是沿着函数的坡（切线）慢慢往下走。现在假设有一对给定的a和b的值，这里用A和B来表示。那么下一个A值

$$ \begin{align}
A' &= A - r * \dfrac{ \partial{d} }{ \partial{a} } \\
&= A - r * \dfrac{1}{n} * \sum{ (2 * (f(a, b) - y) *  \dfrac{ \partial{f(a, b)} }{ \partial{a} }) } \\
&= A - r * \dfrac{1}{n} * \sum{ (2 * (f(a, b) - y) * x) } \\
\end{align}$$

$$ \begin{align}
B' &= B - r * \dfrac{ \partial{d} }{ \partial{b} } \\
&= B - r * \dfrac{1}{n} * \sum{ (2 * (f(a, b) - y) *  \dfrac{ \partial{f(a, b)} }{ \partial{b} }) } \\
&= B - r * \dfrac{1}{n} * \sum{ (2 * (f(a, b) - y)) } \\
\end{align}$$

其实就是个求偏微分啦，上过吴恩达机器学习的课的人就知道他的线性cost function是

$$ d = \dfrac{1}{2n} * \sum{ (f(\theta) - y)^2 } $$

然后它偏微分方程（学习方程就变成了）

$$ {\theta_i}' = \theta_i - r * \dfrac{1}{n} * \sum{ (f(a, b) - y)) * x_i } $$

这个的好处是消去了那个讨厌的常数2。

### 逻辑斯蒂函数

逻辑斯蒂函数是用来对付分类变量的。例如在区分垃圾邮件的应用中，y=1时代表垃圾邮件，y=0时代表非垃圾邮件。这种条件下继续使用线性函数就非常不合适了，因为模型计算出来的值有可能大于1，或者小于0。这些数与目标值之间的距离显得非常不合理。将大于1的都当作1，小于0的都当作0怎么样呢？这在计算代价函数时还是不错的，但是通过偏微分求学习方程时就碰到了大麻烦，这个函数它不连续啊。

逻辑斯蒂方程如下

$$ f(x) = \dfrac{1}{1+e^{-x}} $$

这个函数的特征是自变量越小，结果越接近0，自变量越大，结果越接近1。函数范围在0，1之间，而且是连续的。如果要作为代价函数前需要先改造一下变成

$$ f(a) = \dfrac{1}{1+e^{-a*x}} $$

然后我们定义分类变量的代价函数为

$$ d = -\dfrac{1}{n} * \sum{ y * log{(f(a))} + (1 - y) * log{(1 - f(a))} } $$

由于y只有1和0两种值，可以发现上面式子在y=0时，实际上是预测值到0的平均对数距离，y=1时是预测值到1的平均对数距离。那么当每个种类的预测值都充分接近各自的分类值时，d值就达到其最小值了。实际上这个式子还有一个很厉害的地方，就是它的学习方程和线性方程的学习方程是一样的

$$ a' = a - r * \dfrac{1}{n} * \sum{ (f(a) - y) * x) } $$
