---
layout: post
title:  "SVM"
date: 2014-08-24 13:03:35
categories: "algorithm"
author: xxiieao
---

SVM，全称support vector machine中文名字叫支持向量机。我刚接触时觉得很高大上，感觉和图灵机、冯诺依曼机一样。我接触SVM的时间非常短，在很长一段时间内只是知道有这样东西的存在，而对SVM的了解就仅限于百度百科里有一段资料——“支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面。”我对于这个描述完全是高大上和不明觉厉的节奏。实际上此机并非彼机，虽然名字中有个“机”字，但是支持向量机只是一种最优化算法，本质上和梯度下降算法、遗传算法等等是没有区别的。当SVM同样也属于机器学习的范畴。下面我们就走进科学看看SVM的本质。

首先来看看SVM能做什么？一句话概之，普通监督分类算法能做的，它也能做。

那么普通监督分类算法能做的，为何要用SVM来做？同样一句话回答，因为它做得更好。这里我们用两张图来表示什么叫做更好：

![picture](http://ww1.sinaimg.cn/mw690/6daafd01gw1exrfoaasncj208b06jmxh.jpg)

图1是普通监督分类算法可能会得出的结果，我们可以发现以现有数据来说，这条直线都很好进行了分类。这是因为在梯度下降的过程中，当两类数据被完全分开后，代价函数就不会再移动了，所以直线也就不会再移动了。显然这条直线并不是我们最想要的结果，SVM就能克服这个问题。

![picture](http://ww4.sinaimg.cn/mw690/6daafd01gw1exrfoapgorj208k06jwex.jpg)

如图2所示SVM会把边界划分在距离两类数据距离最大的地方，因此有时候SVM也被称为大边际分类器（large margin classifier），不过这个名字已经暴露了SVM一部分的本质。

最后一个问题，为何SVM能分在这个地方？为啥SVM的名字里有向量一词？先看下面图3，抛开SVM里面复杂的核函数什么的，先看两条边界线，左边一条是普通监督分类算法的结果，右边一条是SVM的结果。先将边界线的参数作为向量（绿虚线）画出来（这个向量是与边界线垂直的，例如左边的边界线是$$X_1-X_2=0$$，边界线参数$$X_1$$和$$X_2$$的系数即[1,1]，而右边的边界线是$$0*X_1+X_2=0$$即[1,0]）。

![picture](http://ww2.sinaimg.cn/mw690/6daafd01gw1exrfoazhtnj20h606c3yz.jpg)

在讨论SVM的神奇效果之前，先说明一下如何利用边界线进行区分红蓝点。例如在左图R1有两个坐标[$$X_1R_1$$,$$X_2R_1$$]，那么他的预测值$$Y$$就等于[$$X_1R_1$$,$$X_2R_1$$]对边界线的参数[1,1]做点乘法再求和，而这个过程实际上等于R1点在绿虚线上的投影（红实线）长度乘以绿虚线本身的长度。这其实就是向量乘积的变换过程，也是SVM里向量一词的由来。

总结上面一点其实就是说红蓝点的区分是以每个点在边界线的垂直线上的投影长度与边界线参数向量的长度的乘积（即$$Y$$）来决定的。相对与普通的监督分类算法，SVM做了一个要求边界线参数向量长度要尽量短。可以想到对于同样的Y值来说，其实就是要求各点投影长度的总和要尽量长（例如$$R_1$$点在左图的投影显然短于右图的投影，左图边界线参数向量的长度为$$sqrt(2)$$，而右图边界线参数向量的长度仅为1）。